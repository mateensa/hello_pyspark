Introduction: Spark is fast, easy to use, engine for Big-Data processing  
    it has below modules or api's
    
    1.  Streaming
    2.  SQL/ETL
    3.  Machine learning
    4.  Graph Processing
    
    Spark Python API aka pyspark, exposes spark model into python programming model using python API
   
Why use spark when (what is) mapreduce is available? 

What is: 

MapReduce is engine responsible for bigdata processing, in distributed way but parallely. it's a part of Handoop eco-system. 
It  has two components map and reduce. 
     Map: It takes set of data from source and converts in to tuples (key value pair)
     Reduce: It takes data from Map and reduce it (by applying transformations or cleansing ... ) parallely across clusters.
     
Why: 
    Map writes data into disks, first maps reads data in serialized format and decerilize it (aka. object encryption or decryption) 
    writes to disk in form of tuples, thus making available for reduce. 
    
    Reduce: reads data from Map and does some transformations and writes data again to disk. 
    
Thus map and reduce totally does io  (read/writes to disk) & processing, it does not consider writing data to some place where it does not have read or write frequently (ram).
To avoid this (call it as feature of map-reduce), Spark has been used which does all in memory processing

